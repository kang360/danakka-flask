import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
import os
import re

current_year = datetime.now().year
current_month = datetime.now().month
# âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼)
load_dotenv()

# âœ… MySQL ì—°ê²° ì„¤ì •
def connect_postgres():
    user = os.getenv("PG_USER")
    password = os.getenv("PG_PASSWORD")
    host = os.getenv("PG_HOST")
    port = os.getenv("PG_PORT", 5432)
    database = os.getenv("PG_DATABASE")
    sslmode = os.getenv("PG_SSL", "require")

    url = f"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}?sslmode={sslmode}"
    engine = create_engine(url, echo=False)
    return engine



# âœ… MySQL í…Œì´ë¸” ìƒì„±
def create_table():
    engine = connect_postgres()
    with engine.connect() as conn:
        conn.execute(text('''
            CREATE TABLE IF NOT EXISTS cruise_schedule (
                id INT AUTO_INCREMENT PRIMARY KEY,
                zone VARCHAR(100) NOT NULL,
                site_name VARCHAR(100) NOT NULL,
                ship_name VARCHAR(100) NOT NULL,
                date DATE NOT NULL,
                wave_power VARCHAR(100),
                fish_name VARCHAR(100),
                reservation VARCHAR(100),
                booking_url TEXT,
                UNIQUE (zone, site_name, ship_name, date)
            )
        '''))
        print("âœ… í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
# âœ… ë‚ ì§œ í˜•ì‹ ë³€í™˜ í•¨ìˆ˜ (YYYY-MM-DD)
def format_date(raw_date, year):
    try:
        # âœ… ì •ê·œì‹ìœ¼ë¡œ ë‚ ì§œ ê°ì§€ (ì—°ë„ í¬í•¨ ë˜ëŠ” ë¯¸í¬í•¨)
        match = re.search(r'(\d{1,2})ì›”\s*(\d{1,2})ì¼(\(\w\))?', raw_date)
        if match:
            month = int(match.group(1))
            day = int(match.group(2))
            formatted_date = f"{year}-{month:02d}-{day:02d}"
            return formatted_date
        else:
            print(f"âŒ ë‚ ì§œ í˜•ì‹ ì¸ì‹ ì‹¤íŒ¨: {raw_date}")
            return None
    except Exception as e:
        print(f"âŒ ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨: {raw_date} - {e}")
        return None

# âœ… í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ë³„ ë©”ì„œë“œ (ë£¨í‚¤ë‚˜í˜¸)
def crawl_site_lukina(site_name, base_url):
    data = []
   

    for month in range(current_month, current_month + 12):
        year = current_year if month <= 12 else current_year + 1
        month = month if month <= 12 else month - 12
        url = f"{base_url}/{year}{month:02d}"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        items = soup.select(".shipsinfo_daywarp")
        for item in items:
            raw_date = item.select_one(".date_info").text.strip().replace("\n", "")
            formatted_date = format_date(raw_date, year)  # âœ… ë‚ ì§œ ë³€í™˜
    
            if not formatted_date:
                continue  # ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
            
            wave_power = item.select_one(".date_info2").text.strip()
            zone = "ì¸ì²œê¶Œ"
            ships = item.select(".small_event_wrap")
            for ship in ships:
                ship_name = ship.select_one(".ship_info>.title").text.strip()
                fish_name = ship.select_one(".fishspecies").text.replace("ì–´ì¢… :", "").split("/")[0].strip() if ship.select_one(".fishspecies") else "[]"
                reservation = ship.select_one(".number.blink_me.n_blue.f_20").text.strip() if ship.select_one(".number.blink_me.n_blue.f_20") else "ë§ˆê°"
                booking_url = f"{base_url}/{year}{month:02d}"
                
                data.append([zone, site_name, ship_name, formatted_date, wave_power, fish_name, reservation, booking_url])
    
    return data
def crawl_site_rise(site_name, base_url):
    data = []
   
    # 12ê°œì›” ë™ì•ˆ í¬ë¡¤ë§
    for month in range(current_month, current_month + 12):
        year = current_year if month <= 12 else current_year + 1
        month = month if month <= 12 else month - 12
        
        # URL ìƒì„±
        url = f"{base_url}/{year}{month:02d}"
        response = requests.get(url)
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')
        items = soup.select(".shipsinfo_daywarp")
        if not items:
            break
        # ë‚ ì§œ ì¶”ì¶œ
        for item in items:
            # ì„ ë°•ëª…
            ship_name = item.select_one(".ship_info>div.title").text.strip()
            zone = "ì¸ì²œê¶Œ"
            # ë‚ ì§œ ë° ì¡°ë¥˜ì„¸ê¸°
            
            raw_date = item.select_one(".date_wrap").text.strip().replace("\n", "")
            formatted_date = format_date(raw_date, year)  # âœ… ë‚ ì§œ ë³€í™˜
            
            if not formatted_date:
                continue  # ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
            
            wave_power = item.select_one(".date_info2").text.strip()

            # ì–´ì¢… ì •ë³´
            fish_name = item.select_one(".fishspecies").text.replace("ì–´ì¢… :", "").split("/")[0].strip()

            # ì˜ˆì•½ ì •ë³´
            reser_img = item.select_one('li.remain').text.strip()
            if "ë‚¨ì€ìë¦¬" in reser_img:
                reservation = item.select_one('li.remain span.number').text.strip()
            else:
                reservation = "ë§ˆê°"
            booking_url = f"{base_url}/{year}{month:02d}"    
            
            # ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            data.append([zone,site_name, ship_name, formatted_date, wave_power, fish_name, reservation, booking_url])
           
    df = pd.DataFrame(data, columns=['ì§€ì—­','ì‚¬ì´íŠ¸', 'ì„ ë°•ëª…', 'ë‚ ì§œ', 'ì¡°ë¥˜ì„¸ê¸°', 'ì–´ì¢…', 'ì˜ˆì•½ìë¦¬','ë°”ë¡œê°€ê¸°'])
    return data

def crawl_site_ottogi(site_name, base_url):

    data = []

    # 12ê°œì›” ë™ì•ˆ í¬ë¡¤ë§
    for month in range(current_month, current_month + 12):
        year = current_year if month <= 12 else current_year + 1
        month = month if month <= 12 else month - 12
        
        # URL ìƒì„±
        url = f"{base_url}/{year}{month:02d}"
        response = requests.get(url)
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')
        items = soup.select(".shipsinfo_daywarp")
      
        # ë‚ ì§œ ì¶”ì¶œ
        for item in items:
        # ë‚ ì§œ ë° ì¡°ë¥˜ì„¸ê¸°
            
            raw_date = item.select_one(".date_wrap").text.strip().replace("\n", "")
            formatted_date = format_date(raw_date, year)  # âœ… ë‚ ì§œ ë³€í™˜
            
            if not formatted_date:
                continue  # ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
            
            
            wave_power = item.select_one(".date_info2").text.strip()
            zone = "ì¸ì²œê¶Œ"
            
            ships = item.select(".ships_warp>table")
            for ship in ships:
                ship_name = ship.select_one(".ship_info>div.title").text.strip()
                fish_element = ship.select_one("#fish")
                if fish_element is not None:
                    fish_name = fish_element.text.strip().replace('[', '').replace(']', '')
                else:
                    fish_name = '[]'
                reservation = ship.select_one('.remain').text.strip()
                if "ë‚¨ì€ìë¦¬" in reservation:
                    reservation = ship.select_one('.number.blink_me').text.strip()
                else:
                    reservation = "ë§ˆê°"
                booking_url = f"{base_url}/{year}{month:02d}"  
                    
                data.append([zone,site_name, ship_name, formatted_date, wave_power, fish_name, reservation, booking_url])
                
    # DataFrame ìƒì„±
    df = pd.DataFrame(data, columns=['ì§€ì—­','ì‚¬ì´íŠ¸', 'ì„ ë°•ëª…', 'ë‚ ì§œ', 'ì¡°ë¥˜ì„¸ê¸°', 'ì–´ì¢…', 'ì˜ˆì•½ìë¦¬','ë°”ë¡œê°€ê¸°'])
    return data
def crawl_site_supernova(site_name, base_url):
    
    data = []

    # 12ê°œì›” ë™ì•ˆ í¬ë¡¤ë§
    for month in range(current_month, current_month + 12):
        year = current_year if month <= 12 else current_year + 1
        month = month if month <= 12 else month - 12
        
        # URL ìƒì„±
        url = f"{base_url}/{year}{month:02d}"
        response = requests.get(url)
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')
        items = soup.select(".shipsinfo_daywarp")
      
        # ë‚ ì§œ ì¶”ì¶œ
        for item in items:
        # ë‚ ì§œ ë° ì¡°ë¥˜ì„¸ê¸°
            
            raw_date = item.select_one(".date_wrap").text.strip().replace("\n", "")
            formatted_date = format_date(raw_date, year)  # âœ… ë‚ ì§œ ë³€í™˜
            
            if not formatted_date:
                continue  # ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
            
            
            wave_power = item.select_one(".date_info2").text.strip()
            zone = "ì¸ì²œê¶Œ"
            
            ships = item.select(".ships_warp>table")
            for ship in ships:
                ship_name = ship.select_one(".ship_info>div.title").text.strip()
                fish_element = ship.select_one("#fish")
                if fish_element is not None:
                    fish_name = fish_element.text.strip().replace('[', '').replace(']', '')
                else:
                    fish_name = '[]'
                reservation = ship.select_one('.remain').text.strip()
                if "ë‚¨ì€ìë¦¬" in reservation:
                    reservation = ship.select_one('.number.blink_me').text.strip()
                else:
                    reservation = "ë§ˆê°"
                booking_url = f"{base_url}/{year}{month:02d}"  
                    
                data.append([zone,site_name, ship_name, formatted_date, wave_power, fish_name, reservation, booking_url])
              
    # DataFrame ìƒì„±
    df = pd.DataFrame(data, columns=['ì§€ì—­','ì‚¬ì´íŠ¸', 'ì„ ë°•ëª…', 'ë‚ ì§œ', 'ì¡°ë¥˜ì„¸ê¸°', 'ì–´ì¢…', 'ì˜ˆì•½ìë¦¬','ë°”ë¡œê°€ê¸°'])
    return data
def crawl_site_bigboss(site_name, base_url):
    
    data = []

    # 12ê°œì›” ë™ì•ˆ í¬ë¡¤ë§
    for month in range(current_month, current_month + 12):
        year = current_year if month <= 12 else current_year + 1
        month = month if month <= 12 else month - 12
        
        # URL ìƒì„±
        url = f"{base_url}/{year}{month:02d}"
        response = requests.get(url)
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')
        items = soup.select(".shipsinfo_daywarp")
               # ë‚ ì§œ ì¶”ì¶œ
        for item in items:
        # ë‚ ì§œ ë° ì¡°ë¥˜ì„¸ê¸°
            
            raw_date = item.select_one(".date_wrap").text.strip().replace("\n", "")
            formatted_date = format_date(raw_date, year)  # âœ… ë‚ ì§œ ë³€í™˜
            
            if not formatted_date:
                continue  # ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
            
            
            wave_power = item.select_one(".date_info2").text.strip()
            zone = "ì¶©ì²­ê¶Œ"
            
            ships = item.select(".ships_warp>table")
            for ship in ships:
                ship_name = ship.select_one(".ship_info>div.title").text.strip()
                fish_element = ship.select_one("#fish")
                if fish_element is not None:
                    fish_name = fish_element.text.strip().replace('[', '').replace(']', '')
                else:
                    fish_name = '[]'
                reservation = ship.select_one('.remain').text.strip()
                if "ë‚¨ì€ìë¦¬" in reservation:
                    reservation = ship.select_one('.number.blink_me').text.strip()
                else:
                    reservation = "ë§ˆê°"
                booking_url = f"{base_url}/{year}{month:02d}"  
                    
                data.append([zone,site_name, ship_name, formatted_date, wave_power, fish_name, reservation, booking_url])
                
    # DataFrame ìƒì„±
    df = pd.DataFrame(data, columns=['ì§€ì—­','ì‚¬ì´íŠ¸', 'ì„ ë°•ëª…', 'ë‚ ì§œ', 'ì¡°ë¥˜ì„¸ê¸°', 'ì–´ì¢…', 'ì˜ˆì•½ìë¦¬','ë°”ë¡œê°€ê¸°'])
    return data 

# âœ… í¬ë¡¤ë§ ë° MySQL ì €ì¥ í•¨ìˆ˜
# âœ… í¬ë¡¤ë§ ë° MySQL ì €ì¥ í•¨ìˆ˜
def crawl_and_save_to_mysql():
    engine = connect_postgres()
    create_table()  # âœ… í…Œì´ë¸” ìƒì„± í™•ì¸

    # í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ ëª©ë¡
    sites = {
        "ë£¨í‚¤ë‚˜í˜¸": "https://lukina.sunsang24.com/ship/schedule_fleet",
        "ì˜ì¢…ë„ë¼ì´ì¦ˆí˜¸": "https://risefishing.sunsang24.com/ship/schedule_fleet",
        "ì˜ì¢…ë„ì˜¤ëšœê¸°í˜¸": "http://ottogi.sunsang24.com/ship/schedule_fleet",
        "ìŠˆí¼ë…¸ë°”": "https://supernova.sunsang24.com/ship/schedule_fleet",
        "ì˜¤ì²œë¹…ë³´ìŠ¤í˜¸": "https://bigboss24.sunsang24.com/ship/schedule_fleet"
    }

    # âœ… ì²« ë²ˆì§¸ ì‚¬ì´íŠ¸ì—ì„œë§Œ ê¸°ì¡´ ë°ì´í„° ì „ì²´ ì‚­ì œ
    first_site = True

    for site_name, base_url in sites.items():
        print(f"ğŸ” {site_name} í¬ë¡¤ë§ ì‹œì‘...")

        # âœ… ì‚¬ì´íŠ¸ë³„ ë°ì´í„° í¬ë¡¤ë§
        if site_name == "ë£¨í‚¤ë‚˜í˜¸":
            site_data = crawl_site_lukina(site_name, base_url)
        elif site_name == "ì˜ì¢…ë„ë¼ì´ì¦ˆí˜¸":
            site_data = crawl_site_rise(site_name, base_url)
        elif site_name == "ì˜ì¢…ë„ì˜¤ëšœê¸°í˜¸":
            site_data = crawl_site_ottogi(site_name, base_url)
        elif site_name == "ìŠˆí¼ë…¸ë°”":
            site_data = crawl_site_supernova(site_name, base_url)
        elif site_name == "ì˜¤ì²œë¹…ë³´ìŠ¤í˜¸":
            site_data = crawl_site_bigboss(site_name, base_url)

        with engine.connect() as conn:
            transaction = conn.begin()  # âœ… íŠ¸ëœì­ì…˜ ì‹œì‘
            try:
                # âœ… ì²« ë²ˆì§¸ ì‚¬ì´íŠ¸ì¼ ê²½ìš°ë§Œ ì „ì²´ ë°ì´í„° ì‚­ì œ + ID ì´ˆê¸°í™”
                if first_site:
                    conn.execute(text('TRUNCATE TABLE cruise_schedule'))
                    print("âœ… ê¸°ì¡´ ë°ì´í„° ì „ì²´ ì‚­ì œ ë° ID ì´ˆê¸°í™”")
                    first_site = False  # ì´í›„ ì‚¬ì´íŠ¸ì—ì„œëŠ” ì‚­ì œí•˜ì§€ ì•ŠìŒ

                # âœ… ìƒˆ ë°ì´í„° ì‚½ì…
                for row in site_data:
                    conn.execute(text('''
                        INSERT INTO cruise_schedule 
                        (zone, site_name, ship_name, date, wave_power, fish_name, reservation, booking_url)
                        VALUES (:zone, :site_name, :ship_name, :date, :wave_power, :fish_name, :reservation, :booking_url)
                    '''), {
                        "zone": row[0],
                        "site_name": row[1],
                        "ship_name": row[2],
                        "date": row[3],
                        "wave_power": row[4],
                        "fish_name": row[5],
                        "reservation": row[6],
                        "booking_url": row[7]
                    })

                transaction.commit()  # âœ… ëª…ì‹œì  ì»¤ë°‹
                print(f"âœ… {site_name} ë°ì´í„° ì €ì¥ ì™„ë£Œ")

            except Exception as e:
                transaction.rollback()  # âœ… ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡¤ë°±
                print(f"âŒ ì €ì¥ ì˜¤ë¥˜: {e}")


    # âœ… DBì—ì„œ ë°ì´í„° ì½ì–´ì™€ ì¶œë ¥
    with engine.connect() as conn:
        df = pd.read_sql('SELECT * FROM cruise_schedule ORDER BY date DESC', conn)
    
    return df

def run_crawler():
    df = crawl_and_save_to_mysql()
    print(df)

